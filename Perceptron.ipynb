{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No notebook anterior, nós aprendemos intuitivamente como o perceptron aprende. De maneira geral, nós vamos atualizando os pesos e o bias sempre buscando diminuir uma função de custo. Nesse notebook, nós vamos ver como esse aprendizado realmente acontence, tanto na teoria quanto na prática. Também utilizaremos o Perceptron para resolver problemas de classificação e regressão.\n",
    "\n",
    "__Objetivos__:\n",
    "\n",
    "- Implementar o perceptron e seu modelo de aprendizado em Python puro e Numpy\n",
    "- Utilizar o perceptron para regressão e classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Introdução](#Introdução)\n",
    "- [Regra de Aprendizado do Perceptron](#Regra-de-Aprendizado-do-Perceptron)\n",
    "- [Pseudo-algoritmo do Perceptron](#Pseudo-algoritmo-do-Perceptron)\n",
    "\n",
    "[Classificação](#Classificação)\n",
    "- [Porta AND/OR](#Porta-AND/OR)\n",
    "- [Exercício de Classificação](#Exerc%C3%ADcio-de-Classificação)\n",
    "\n",
    "[Regressão](#Regressão)\n",
    "- [Exercício de Regressão](#Exerc%C3%ADcio-de-Regressão)\n",
    "\n",
    "[Referências](#Referências)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-20T12:53:30.345746Z",
     "start_time": "2017-09-20T12:52:48.057739Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from random import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "O tipo mais básico de Rede Neural Artificial é formada por apenas um neurônio, o __Perceptron__. Inicialmente, o Perceptron foi projetado para ser um __classificador binário linear__ responsável por mapear uma ou mais entradas em uma saída desejada. Porém, também podemos utilizá-lo para resolver problemas de __regressão linear__. Ele foi projetado em 1957 por Frank Rosenblatt.\n",
    "\n",
    "O perceptron é formado por:\n",
    "\n",
    "<img src='images/perceptron.png' width='350'>\n",
    "\n",
    "- __entradas__ $x_1,...,x_D$: representam os atributos dos seus dados com dimensionalidade $D$. O Perceptron aceita qualquer tamanho de entrada, porém a saída é sempre apenas um valor.\n",
    "- __junção aditiva__ $\\sum$: também chamada de _função agregadora_, nada mais é que a soma ponderada das entradas com os __pesos__ ($w_1,...,w_D)$. Em geral, o resultado é somado com um __bias__ $b$, responsável por deslocar o resultado do somatório. A junção aditiva é descrita pela seguinte fórmula:\n",
    "\n",
    "$$\\sum_i^D{x_iw_i} + b$$\n",
    "\n",
    "- __função de ativação__ $f$: utilizada para mapear o resultado da junção aditiva em uma saída esperada. Mais detalhes abaixo.\n",
    "\n",
    "Logo, o Perceptron é representado pela seguinte fórmula matemática:\n",
    "\n",
    "$$\\widehat{y}_i = f(\\sum_i^D{x_iw_i} + b)$$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $D$: representa a dimensionalidade das amostras, ou seja, a quantidade de atributos de cada amostra.\n",
    "- $x_i$: representam os atributos de uma amostra que servem de entrada para o Perceptron.\n",
    "- $w_i$: representam os __pesos sinápticos__ que ponderam as entradas.\n",
    "- $b$: representa o __bias__, responsável por deslocar a fronteira de decisão além da origem e não depende de nenhum valor de entrada. Repare que o bias encontra-se fora do somatório.\n",
    "- $f$: __função de ativação__. Quando a função de ativação é linear, ou seja, nenhuma transformação é aplicada no resultado da junção aditiva, o Perceptron atua como um __Regressor Linear__. Se precisamos efetuar uma __Classificação binária__, devemos utilizar a função _step_ (também conhecida como _função degrau_) para mapear a saída em um valor discreto (0 ou 1):\n",
    "\n",
    "$$f = \\begin{cases}1 & se \\ wx+b > 0\\\\0 & caso \\ contr\\acute ario\\end{cases}$$\n",
    "\n",
    "- $\\widehat{y}$: representa a saída do Perceptron (o valor predito).\n",
    "\n",
    "__Observações importantes__:\n",
    "\n",
    "- O Perceptron não faz __Classificação Multiclasse__.\n",
    "- __A atualização dos pesos é *online*, ou seja, efetuada amostra a amostra__ utilizando uma fórmula pré-definida que veremos na seção a seguir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regra de Aprendizado do Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Perceptron tem sua própria forma de aprendizado conforme definido no seu artigo original. Na verdade, a fórmula para atualização dos pesos e bias é bem simples:\n",
    "\n",
    "$$w_i = w_i + \\lambda(y_i - \\widehat{y}_i)x_i$$\n",
    "<br>\n",
    "$$b_i = b_i + \\lambda(y_i - \\widehat{y}_i)$$\n",
    "\n",
    "Onde $\\lambda$ é a __taxa de aprendizagem__ (___learning rate___).\n",
    "\n",
    "Repare que $y_i - \\widehat{y}_i$ significa calcular a diferença entre o valor esperado ($y_i$) e o valor predito ($\\widehat{y}_i$). Supondo que estamos fazendo __classificação binária__ de uma amostra $(x_i, y_i)$. Nesse caso, teremos duas possibilidades:\n",
    "- __O valor esperado é $y_i = \\widehat{y}_i$__, ou seja, a saída do Perceptron (após a função de ativação _step_) é __igual__ a saída esperada. Nesse caso, __a diferença $y_i - \\widehat{y}_i = 0$ e não haverá atualização de pesos__.\n",
    "- __O valor esperado é $y_i \\neq \\widehat{y}_i$__, ou seja, a saída do Perceptron (após a função de ativação _step_) é __diferente__ da saída esperada. Nesse caso, __a atualização dos pesos será dada pela diferença $y_i - \\widehat{y}_i$__. Repare que:\n",
    "    - quando essa diferença é __negativa__ (ou seja, $y_i = 0$ e $\\widehat{y}_i = 1$), __os pesos tendem a diminuir__.\n",
    "    - quando essa diferença é __positiva__ (ou seja, $y_i = 1$ e $\\widehat{y}_i = 0$), __os pesos tendem a aumentar__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-algoritmo do Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Inicialize os pesos $w$ e o bias $b$\n",
    "2. Para cada amostra $(x_n, y_n)$ do nosso banco:\n",
    "    1. Calcule $\\widehat{y} = f(\\sum_i^D{x_iw_i} + b)$, onde $f$ é a __função _step_ para classificação__ e __linear no caso da regressão__\n",
    "    2. Calcule o $erro = y_n - \\widehat{y}$\n",
    "    3. Atualize os pesos $w_i = w_i + \\lambda*erro*x_i$\n",
    "    4. Atualize o bias $b_i = b_i + \\lambda*erro$\n",
    "3. Repita o passo 2 por N vezes ou até que alguma medida de custo para o $erro$ seja menor que um valor pré-determinado.\n",
    "    \n",
    "Repare, como dito lá em cima, que __a atualização dos pesos e bias é feito a cada amostra__, e não somente após ver todas as amostras do banco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porta AND/OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:11:37.370366Z",
     "start_time": "2017-09-15T11:11:37.359356Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "#y = np.array([0, 1, 1, 1]) # porta OR\n",
    "y = np.array([0, 0, 0, 1]).T # porta AND\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:21:18.798586Z",
     "start_time": "2017-09-15T11:21:18.667487Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T12:21:02.603975Z",
     "start_time": "2017-09-15T12:21:02.555936Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercício de Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_blobs(n_samples=100, n_features=2, centers=2, random_state=1234)\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "plt.scatter(x[:,0], x[:,1], c=y.ravel(), cmap='bwr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_linear_classifier(x, y, w, b):\n",
    "    x1_min, x1_max = x[:,0].min(), x[:,0].max()\n",
    "    x2_min, x2_max = x[:,1].min(), x[:,1].max()\n",
    "\n",
    "    x1, x2 = np.meshgrid(np.linspace(x1_min-1, x1_max+1,100), np.linspace(x2_min-1, x2_max+1, 100))\n",
    "    x_mesh = np.array([x1.ravel(), x2.ravel()]).T\n",
    "\n",
    "    plt.scatter(x[:,0], x[:,1], c=y.ravel(), cmap='bwr')\n",
    "\n",
    "    y_mesh = np.dot(x_mesh, np.array(w).reshape(1, -1).T) + b\n",
    "    y_mesh = np.where(y_mesh <= 0, 0, 1)\n",
    "\n",
    "    plt.contourf(x1, x2, y_mesh.reshape(x1.shape), cmap='bwr', alpha=0.5)\n",
    "    plt.xlim(x1_min-1, x1_max+1)\n",
    "    plt.ylim(x2_min-1, x2_max+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regressão "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Para transformar o Perceptron em um __regressor linear__, só o que temos de fazer é __remover a função de ativação _step___, transformando-a em uma função de ativação linear.\n",
    "\n",
    "Apesar dessa modificação, __a fórmula de atualização dos pesos não sofre nenhuma alteração__. \n",
    "\n",
    "Vamos, então, implementar nosso perceptron para classificação em Python, Numpy, Keras e TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T19:21:04.802972Z",
     "start_time": "2017-09-14T19:21:04.773952Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/medidas.csv')\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T19:21:08.765341Z",
     "start_time": "2017-09-14T19:21:08.441110Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = df.Altura.values\n",
    "y = df.Peso.values\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('Altura')\n",
    "plt.ylabel('Peso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T19:21:10.893855Z",
     "start_time": "2017-09-14T19:21:10.883847Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T19:21:11.535313Z",
     "start_time": "2017-09-14T19:21:11.527304Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = x.reshape(-1, 1)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__Exercício__: tentar estimar as learning_rates de **w** e __b__. Elas são diferentes por que nossos dados não estão na mesma escala!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T19:21:38.253347Z",
     "start_time": "2017-09-14T19:21:16.413722Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D = x.shape[1]\n",
    "w = [2*random() - 1 for i in range(D)]\n",
    "b = 2*random() - 1\n",
    "\n",
    "for step in range(10001):\n",
    "    cost = 0\n",
    "    for x_n, y_n in zip(x, y):\n",
    "        # qual linha devemos remover para transformar o Perceptron num regressor?\n",
    "        y_pred = sum([x_i*w_i for x_i, w_i in zip(x_n, w)]) + b\n",
    "        y_pred = 1 if y_pred > 0 else 0\n",
    "        error = y_n - y_pred\n",
    "        w = [w_i + 1.0*error*x_i for x_i, w_i in zip(x_n, w)]\n",
    "        b = b + 1.0*error\n",
    "        cost += error**2\n",
    "\n",
    "    if step%1000 == 0:\n",
    "        print('step {0}: {1}'.format(step, cost))\n",
    "\n",
    "print('w: ', w)\n",
    "print('b: ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T19:21:45.406815Z",
     "start_time": "2017-09-14T19:21:45.008532Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D = x.shape[1]\n",
    "w = 2*np.random.random(size=D)-1\n",
    "b = 2*np.random.random()-1 \n",
    "\n",
    "for step in range(10001):\n",
    "    cost = 0\n",
    "    for x_n, y_n in zip(x, y):\n",
    "        # qual linha devemos remover para transformar o Perceptron num regressor?\n",
    "        y_pred = np.dot(x_n, w) + b \n",
    "        y_pred = np.where(y_pred > 0, 1, 0)\n",
    "        error = y_n - y_pred\n",
    "        w = w + 1.0*np.dot(error, x_n)\n",
    "        b = b + 1.0*error\n",
    "        cost += error**2\n",
    "    \n",
    "    if step%1000 == 0:\n",
    "        print('step {0}: {1}'.format(step, cost))\n",
    "    \n",
    "print('w: ', w)\n",
    "print('b: ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Numpy com Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T19:22:08.568244Z",
     "start_time": "2017-09-14T19:22:08.561239Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler(feature_range=(-1,1))\n",
    "x = minmax.fit_transform(x.astype(np.float64))\n",
    "\n",
    "print(x.min(), x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(x,y)\n",
    "\n",
    "print('w: ', reg.coef_)\n",
    "print('b: ', reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T19:22:33.763665Z",
     "start_time": "2017-09-14T19:22:33.556518Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D = x.shape[1]\n",
    "w = 2*np.random.random(size=D)-1\n",
    "b = 2*np.random.random()-1 \n",
    "\n",
    "learning_rate = 1.0 # <- tente estimar a learning_rate\n",
    "\n",
    "for step in range(1001):\n",
    "    cost = 0\n",
    "    for x_n, y_n in zip(x, y):\n",
    "        y_pred = np.dot(x_n, w) + b \n",
    "        error = y_n - y_pred\n",
    "        w = w + learning_rate*np.dot(error, x_n)\n",
    "        b = b + learning_rate*error\n",
    "        cost += error**2\n",
    "    \n",
    "    if step%100 == 0:\n",
    "        print('step {0}: {1}'.format(step, cost))\n",
    "    \n",
    "print('w: ', w)\n",
    "print('b: ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Exercício de Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T10:56:07.079178Z",
     "start_time": "2017-09-15T10:56:06.991114Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/notas.csv')\n",
    "\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(df.prova1.values, df.final.values)\n",
    "plt.xlabel('Prova 1')\n",
    "plt.ylabel('Final')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(df.prova2.values, df.final.values)\n",
    "plt.xlabel('Prova 2')\n",
    "plt.ylabel('Final')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(df.prova3.values, df.final.values)\n",
    "plt.xlabel('Prova 3')\n",
    "plt.ylabel('Final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T10:56:14.202826Z",
     "start_time": "2017-09-15T10:56:14.189835Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = df[['prova1', 'prova2', 'prova3']].values\n",
    "y = df['final'].values\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T10:56:15.149753Z",
     "start_time": "2017-09-15T10:56:15.143751Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler(feature_range=(-1,1))\n",
    "x = minmax.fit_transform(x.astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T19:24:23.821886Z",
     "start_time": "2017-09-14T19:24:23.678784Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(x, y)\n",
    "\n",
    "print('w: ', reg.coef_)\n",
    "print('b: ', reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T19:24:36.348182Z",
     "start_time": "2017-09-14T19:24:33.850407Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D = x.shape[1]\n",
    "w = [2*random() - 1 for i in range(D)]\n",
    "b = 2*random() - 1\n",
    "\n",
    "learning_rate = 1.0 # <- tente estimar a learning_rate\n",
    "\n",
    "for step in range(1): # <- tente estimar o número de passos\n",
    "    cost = 0\n",
    "    for x_n, y_n in zip(x, y):\n",
    "        y_pred = sum([x_i*w_i for x_i, w_i in zip(x_n, w)]) + b\n",
    "        error = y_n - y_pred\n",
    "        w = [w_i + learning_rate*error*x_i for x_i, w_i in zip(x_n, w)]\n",
    "        b = b + learning_rate*error\n",
    "        cost += error**2\n",
    "        \n",
    "    if step%200 == 0:\n",
    "        print('step {0}: {1}'.format(step, cost))\n",
    "\n",
    "print('w: ', w)\n",
    "print('b: ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T19:24:55.296538Z",
     "start_time": "2017-09-14T19:24:54.907259Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D = x.shape[1]\n",
    "w = 2*np.random.random(size=D)-1\n",
    "b = 2*np.random.random()-1       \n",
    "\n",
    "learning_rate = 1.0 # <- tente estimar a learning_rate\n",
    "\n",
    "for step in range(1): # <- tente estimar o número de passos\n",
    "    cost = 0\n",
    "    for x_n, y_n in zip(x, y):\n",
    "        y_pred = np.dot(x_n, w) + b \n",
    "        error = y_n - y_pred\n",
    "        w = w + learning_rate*np.dot(error, x_n)\n",
    "        b = b + learning_rate*error\n",
    "        cost += error**2\n",
    "    \n",
    "    if step%200 == 0:\n",
    "        print('step {0}: {1}'.format(step, cost))\n",
    "    \n",
    "print('w: ', w)\n",
    "print('b: ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Artigo original do Perceptron](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
