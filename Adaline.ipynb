{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No notebook anterior, nós aprendemos sobre o Perceptron. Vimos como ele aprende e como pode ser utilizado tanto para classificação binária quanto para regressão linear. Nesse notebook, nós veremos um algoritmo muito parecido com o Perceptron, mais conhecido como __Adaline__, que foi uma proposta de melhoria ao algoritmo original do Perceptron. Veremos as semelhanças e diferenças entre os dois algoritmos e iremos implementá-lo utilizando python e numpy. Por fim, vamos aplicar nos mesmos problemas de classificação do notebook do Perceptron para entender de fato suas diferenças. __O código para utilizar o Adaline em problemas de regressão é exatamente o mesmo do perceptron__.\n",
    "\n",
    "__Objetivos__:\n",
    "\n",
    "- Entender as diferenças entre os algoritmos do Perceptron e Adaline.\n",
    "- Implementar o Adaline e seu modelo de aprendizado em Python puro e Numpy\n",
    "- Utilizar o Adaline para classificação e regressão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Introdução](#Introdução)\n",
    "\n",
    "[Regra de Aprendizado do Adaline](#Regra-de-Aprendizado-do-Adaline)\n",
    "\n",
    "[Classificação](#Classificação)\n",
    "- [Porta AND/OR](#Porta-AND/OR)\n",
    "- [Exercício de Classificação](#Exerc%C3%ADcio-de-Classificação)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-20T12:53:30.345746Z",
     "start_time": "2017-09-20T12:52:48.057739Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from random import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poucos meses após a publicação do teorema da convergência do Perceptron por Rosenblatt, os engenheiros da Universidade de Stanford, Bernard Widrow e Marcian Hoff, publicaram um trabalho descrevendo uma rede neural muito parecida com o Perceptron, a __Adaline__ (do inglês _ADAptive LINear Element_). Porém, ao invés de utilizar a função _step_ como função de ativação, a __Adaline utiliza função de ativação linear e tem uma nova regra de aprendizado supervisionado__, conhecida como __regra de Widrow-Hoff__ (ou __regra delta__, ou ainda __regra LMS__). \n",
    "\n",
    "De fato, tanto o Perceptron quanto o Adaline possuem muitas características semelhantes e __é comum ver o pessoal confundindo o Perceptron com o Adaline__. Entre as principais semelhanças, podemos destacar:\n",
    "- Ambos possuem __apenas um neurônio de N entradas e apenas uma saída. Não há camadas escondidas__.\n",
    "- Ambos são __classificadores lineares binários__ por definição, mas podemos adaptá-los para efetuar __regressão linear__, da mesma forma como vimos no notebook sobre o Perceptron. __Na verdade, o código para treinar um Adaline para regressão é o mesmo de um Perceptron__.\n",
    "- Ambos tem o **método de aprendizagem _online_**. Isto é, a atualização dos pesos é efetuada amostra por amostra.\n",
    "- Ambos tem uma **função _step_ para classificação**. Porém, ao contrário do Perceptron, __na Adaline ela não é utilizada na atualização dos pesos__. Nós veremos por que a seguir.\n",
    "\n",
    "Porém, a principal diferença entre o Perceptron e a Adaline é que o Perceptron utiliza os labels das classes para fazer a atualização dos pesos, enquanto __a Adaline utiliza o resultado da função de ativação (linear) como valor contínuo de predição__. Isto é, ao invés da saída ser discreta como no Perceptron (0 ou 1), __na Adaline a saída pode ser qualquer valor contínuo__. Essa diferença fica mais clara quando vemos a figura a seguir:\n",
    "\n",
    "<img src=\"images/comparacao_perceptron_adaline.png\">\n",
    "[Fonte](https://www.quora.com/What-is-the-difference-between-a-Perceptron-Adaline-and-neural-network-model)\n",
    "\n",
    "Repare, como dito, que ambos têm a função _step_. No Perceptron, ela é utilizada como função de ativação. No Adaline, por sua vez, a função de ativação é linear e a funcão _step_ é utilizada para gerar a predição. \n",
    "\n",
    "Por calcular a saída como um valor contínuo, __muitos consideram o Adaline mais poderoso__, uma vez que a diferença entre a saída desejada e o valor predito ($y_i - \\widehat{y}_i$) nos diz agora \"o quanto estamos certos ou errados\". __Na prática, isso faz com o que o Adaline tente encontrar a \"melhor solução\" para o problema, ao invés de somente uma \"solução adequada\"__. Tomando como exemplo a figura abaixo, o Perceptron pode encontrar diversas retas que separam as classes, enquanto o Adaline tenta encontrar a melhor reta que separa as classes.\n",
    "\n",
    "<img src=\"images/hiperplanos_perceptron_adaline.png\" width='700'>\n",
    "\n",
    "[Fonte](http://www.barbon.com.br/wp-content/uploads/2013/08/RNA_Aula4.pdf)\n",
    "\n",
    "Ok, mas como isso muda o aprendizado? É o que veremos a seguir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Regra de Aprendizado do Adaline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A atualização dos pesos do Adaline é dada pela mesma fórmula do Perceptron:\n",
    "\n",
    "$$w_i = w_i + \\lambda(y_i - \\widehat{y}_i)x_i$$\n",
    "\n",
    "Onde $\\lambda$ é a __taxa de aprendizagem__.\n",
    "\n",
    "Mas você já imaginou da onde vem essa fórmula? Em primeiro lugar, o método de atualização dos pesos é baseado na __Regra Delta__ (*Delta Rule*). Sendo $\\overrightarrow{w} = \\{w_1, w_2, ..., w_D\\}$, a atualização dos pesos é dada por:\n",
    "\n",
    "$$\\overrightarrow{w} = \\overrightarrow{w} - \\Delta{\\overrightarrow{w}}$$\n",
    "\n",
    "em que:\n",
    "\n",
    "$$\\Delta{\\overrightarrow{w}} = \\lambda\\nabla E(\\overrightarrow{w})$$\n",
    "\n",
    "Sendo $\\nabla E(\\overrightarrow{w})$ o gradiente de uma função que depende de $\\overrightarrow{w}$ e que queremos minimizar.\n",
    "\n",
    "No caso do Adaline, __a função de custo é dada pela soma dos erros quadrados__:\n",
    "\n",
    "$$J(w) = \\frac{1}{2}\\sum_{i}^N (y_i - \\widehat{y}_i)^2$$\n",
    "\n",
    "Onde $N$ é a quantidade de amostras nos dados, e as demais variáveis representam as mesmas vistas anteriormente. Repare que a função de custo é quase uma _Mean Squared Error (MSE)_, só que ao invés de dividir por $N$, estamos dividindo por 2 o resultado do somatório. O por quê disso será entendido mais a frente na demonstração.\n",
    "\n",
    "Queremos encontrar, então, o vetor $\\overrightarrow{w}$ que minimiza a função $J$. Assim, temos:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_i} = \\frac{\\partial}{\\partial w_i}\\frac{1}{2}\\sum_i^N (y_i - \\widehat{y}_i)^2$$\n",
    "\n",
    "Como a derivada do somatório é igual ao somatório das derivadas:\n",
    "\n",
    "$$= \\frac{1}{2}\\sum_i^N \\frac{\\partial}{\\partial w_i}(y_i - \\widehat{y}_i)^2$$\n",
    "\n",
    "Aplicando a regra da cadeia:\n",
    "\n",
    "$$= \\sum_i^N (y_i - \\widehat{y}_i)\\frac{\\partial}{\\partial w_i}(y_i - \\widehat{y}_i)$$\n",
    "\n",
    "Repare que, quando derivamos $(y_i - \\widehat{y}_i)^2$, o expoente 2, ao sair do somatório, foi multiplicado por $\\frac{1}{2}$, tornando-o 1. Isso é o que os matemáticos denominam de \"conveniência matemática\". \n",
    "\n",
    "Como $\\widehat{y}_i = x_iw_i + b$ é uma função que depende de $w$, e sua derivada em relação a $w_i$ é apenas $x_i$, temos que:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_i} = \\sum_i^N (y_i - \\widehat{y}_i)(-x_i)$$\n",
    "$$\\frac{\\partial J}{\\partial w_i} = -\\sum_i^N (y_i - \\widehat{y}_i)x_i$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\overrightarrow{w}} = -(\\overrightarrow{y} - \\overrightarrow{\\widehat{y}_i})\\overrightarrow{x}$$\n",
    "\n",
    "De maneira análoga, podemos calcular que a derivada de $J$ em relação a $b_i$ é:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b_i} = -\\sum_i^N (y_i - \\widehat{y}_i)*1.0$$\n",
    "\n",
    "Já que a derivada de $\\widehat{y}_i$ em relação a $b_i$ ($\\frac{\\partial J}{\\partial b_i}$) é igual a 1.0. Logo, a atualização dos bias será dada por:\n",
    "\n",
    "$$b_i = b_i + \\lambda(y_i - \\widehat{y}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/notas.csv')\n",
    "\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[['prova1', 'prova2', 'prova3']].values\n",
    "y = df['final'].values.reshape(-1, 1)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler(feature_range=(-1,1))\n",
    "x = minmax.fit_transform(x.astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = x.shape[1]\n",
    "w = [2*random() - 1 for i in range(D)]\n",
    "b = 2*random() - 1\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "for step in range(2001):\n",
    "    cost = 0\n",
    "    for x_n, y_n in zip(x, y):\n",
    "        y_pred = sum([x_i*w_i for x_i, w_i in zip(x_n, w)]) + b\n",
    "        error = y_n - y_pred\n",
    "        w = [w_i + learning_rate*error*x_i for x_i, w_i in zip(x_n, w)]\n",
    "        b = b + learning_rate*error\n",
    "        cost += error**2\n",
    "        \n",
    "    if step%200 == 0:\n",
    "        print('step {0}: {1}'.format(step, cost))\n",
    "\n",
    "print('w: ', w)\n",
    "print('b: ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porta AND/OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:11:37.370366Z",
     "start_time": "2017-09-15T11:11:37.359356Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "# y = np.array([[0, 1, 1, 1]]).T # porta OR\n",
    "y = np.array([0, 0, 0, 1]).T # porta AND\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:21:18.798586Z",
     "start_time": "2017-09-15T11:21:18.667487Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D = x.shape[1]\n",
    "w = [2*random() - 1 for i in range(D)]\n",
    "b = 2*random() - 1\n",
    "\n",
    "learning_rate = 1.0 # <- tente estimar a learning_rate\n",
    "\n",
    "for step in range(101):\n",
    "    cost = 0\n",
    "    for x_n, y_n in zip(x, y):\n",
    "        # qual linha devemos remover para transformar o Perceptron num Adaline?\n",
    "        y_pred = sum([x_i*w_i for x_i, w_i in zip(x_n, w)]) + b\n",
    "        y_pred = 1 if y_pred > 0 else 0\n",
    "        error = y_n - y_pred\n",
    "        w = [w_i + learning_rate*error*x_i for x_i, w_i in zip(x_n, w)]\n",
    "        b = b + learning_rate*error\n",
    "        cost += error**2\n",
    "        \n",
    "    if step%10 == 0:\n",
    "        print('step {0}: {1}'.format(step, cost))\n",
    "\n",
    "print('w: ', w)\n",
    "print('b: ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T12:21:02.603975Z",
     "start_time": "2017-09-15T12:21:02.555936Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D = x.shape[1]\n",
    "w = 2*np.random.random(size=D)-1\n",
    "b = 2*np.random.random()-1       \n",
    "\n",
    "learning_rate = 1.0 # <- use a mesma learning rate do python\n",
    "\n",
    "for step in range(101):\n",
    "    cost = 0\n",
    "    for x_n, y_n in zip(x, y):\n",
    "        # qual linha devemos remover para transformar o Perceptron num Adaline?\n",
    "        y_pred = np.dot(x_n, w) + b \n",
    "        y_pred = np.where(y_pred > 0, 1, 0)\n",
    "        error = y_n - y_pred\n",
    "        w = w + learning_rate*np.dot(error, x_n)\n",
    "        b = b + learning_rate*error\n",
    "        cost += error**2\n",
    "    \n",
    "    if step%10 == 0:\n",
    "        print('step {0}: {1}'.format(step, cost))\n",
    "    \n",
    "print('w: ', w)\n",
    "print('b: ', b)\n",
    "print('y_pred: {0}'.format(np.dot(x, w)+b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercício de Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_blobs(n_samples=100, n_features=2, centers=2, random_state=1234)\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "plt.scatter(x[:,0], x[:,1], c=y.ravel(), cmap='bwr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_linear_classifier(x, y, w, b):\n",
    "    x1_min, x1_max = x[:,0].min(), x[:,0].max()\n",
    "    x2_min, x2_max = x[:,1].min(), x[:,1].max()\n",
    "\n",
    "    x1, x2 = np.meshgrid(np.linspace(x1_min-1, x1_max+1,100), np.linspace(x2_min-1, x2_max+1, 100))\n",
    "    x_mesh = np.array([x1.ravel(), x2.ravel()]).T\n",
    "\n",
    "    plt.scatter(x[:,0], x[:,1], c=y.ravel(), cmap='bwr')\n",
    "\n",
    "    y_mesh = np.dot(x_mesh, np.array(w).reshape(1, -1).T) + b\n",
    "    y_mesh = np.where(y_mesh < 0.5, 0, 1)\n",
    "\n",
    "    plt.contourf(x1, x2, y_mesh.reshape(x1.shape), cmap='bwr', alpha=0.5)\n",
    "    plt.xlim(x1_min-1, x1_max+1)\n",
    "    plt.ylim(x2_min-1, x2_max+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "D = x.shape[1]\n",
    "w = [2*random() - 1 for i in range(D)]\n",
    "b = 2*random() - 1\n",
    "\n",
    "learning_rate = 1.0 # <- tente estimar a learning_rate\n",
    "\n",
    "for step in range(1): # <- tente estimar a #epochs\n",
    "    cost = 0\n",
    "    for x_n, y_n in zip(x, y):\n",
    "        y_pred = sum([x_i*w_i for x_i, w_i in zip(x_n, w)]) + b\n",
    "        error = y_n - y_pred\n",
    "        w = [w_i + learning_rate*error*x_i for x_i, w_i in zip(x_n, w)]\n",
    "        b = b + learning_rate*error\n",
    "        cost += error**2\n",
    "        \n",
    "    if step%100 == 0:\n",
    "        print('step {0}: {1}'.format(step, cost))\n",
    "\n",
    "print('w: ', w)\n",
    "print('b: ', b)\n",
    "\n",
    "plot_linear_classifier(x, y, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = x.shape[1]\n",
    "w = 2*np.random.random(size=D)-1\n",
    "b = 2*np.random.random()-1       \n",
    "\n",
    "learning_rate = 1.0 # <- use a mesma learning rate do python\n",
    "\n",
    "for step in range(1): # <- use a mesma #epochs do python\n",
    "    cost = 0\n",
    "    for x_n, y_n in zip(x, y):\n",
    "        y_pred = np.dot(x_n, w) + b \n",
    "        error = y_n - y_pred\n",
    "        w = w + learning_rate*np.dot(error, x_n)\n",
    "        b = b + learning_rate*error\n",
    "        cost += error**2\n",
    "    \n",
    "    if step%100 == 0:\n",
    "        print('step {0}: {1}'.format(step, cost))\n",
    "    \n",
    "print('w: ', w)\n",
    "print('b: ', b)\n",
    "\n",
    "plot_linear_classifier(x, y, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [http://sisne.org/Disciplinas/PosGrad/PsicoConex/aula6.pdf](http://sisne.org/Disciplinas/PosGrad/PsicoConex/aula6.pdf)\n",
    "- [What is the difference between a Perceptron, Adaline, and neural network model?](https://www.quora.com/What-is-the-difference-between-a-Perceptron-Adaline-and-neural-network-model)\n",
    "- [RNA – Adaline e Regra do Delta](http://www.barbon.com.br/wp-content/uploads/2013/08/RNA_Aula4.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
